{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Configurar o Spark e Criar a sessão"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "your 131072x1 screen size is bogus. expect trouble\n",
      "25/03/14 12:15:23 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import regexp_replace, col, explode\n",
    "from pyspark.sql.types import DateType\n",
    "from dotenv import load_dotenv\n",
    "import glob\n",
    "import os\n",
    "\n",
    "os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-8-openjdk-amd64\"\n",
    "os.environ[\"SPARK_HOME\"] = \"/opt/spark/spark-3.5.5-bin-hadoop3\"\n",
    "\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"Teste Minsait\") \\\n",
    "    .master(\"local[*]\") \\\n",
    "    .config(\"spark.jars\", \"jars/postgresql-42.7.5.jar\") \\\n",
    "    .config(\"spark.sql.debug.maxToStringFields\", \"200\") \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Configurar conexão com PostgreSQL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "load_dotenv()\n",
    "\n",
    "jdbc_url = f\"jdbc:postgresql://{os.getenv('DB_HOST')}:{os.getenv('DB_PORT')}/{os.getenv('DB_NAME')}\"\n",
    "properties = {\n",
    "    \"user\": os.getenv('DB_USER'),\n",
    "    \"password\": os.getenv('DB_PASSWD'),\n",
    "    \"driver\": \"org.postgresql.Driver\"\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Testar conexão"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------------+\n",
      "|now                       |\n",
      "+--------------------------+\n",
      "|2025-03-14 12:15:27.845356|\n",
      "+--------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "query = \"(SELECT now()) AS query\"\n",
    "spark.read.jdbc(url=jdbc_url, table=query, properties=properties).show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Listar todos os arquivos JSON na pasta 'data' e configurar tamanho do lote de processamento"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "json_files = sorted(glob.glob(\"data/*.json\"))\n",
    "batch_size = 1000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Função para processar um lote de arquivos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_batch(file_batch, df_existing_patients, df_existing_conditions, df_existing_medications):\n",
    "    # Ler o lote de arquivos JSON\n",
    "    df = spark.read.option(\"multiline\", \"true\").json(file_batch)\n",
    "\n",
    "    # Explodir a coluna 'entry'\n",
    "    entries_df = df.select(explode(col(\"entry\")).alias(\"entry\"))\n",
    "\n",
    "    # Extrair recursos e criar DataFrames\n",
    "    patients_df = entries_df.filter(col(\"entry.resource.resourceType\") == \"Patient\").select(\n",
    "        col(\"entry.resource.id\").alias(\"id\"),\n",
    "        col(\"entry.resource.gender\").alias(\"gender\"),\n",
    "        col(\"entry.resource.birthDate\").cast(DateType()).alias(\"birth_date\")\n",
    "    )\n",
    "\n",
    "    conditions_df = entries_df.filter(col(\"entry.resource.resourceType\") == \"Condition\").select(\n",
    "        col(\"entry.resource.id\").alias(\"id\"),\n",
    "        regexp_replace(col(\"entry.resource.subject.reference\"), \"urn:uuid:\", \"\").alias(\"patient_id\"),\n",
    "        col(\"entry.resource.code.text\").alias(\"condition_text\"),\n",
    "        col(\"entry.resource.recordedDate\").cast(DateType()).alias(\"recorded_date\")\n",
    "    )\n",
    "\n",
    "    medications_df = entries_df.filter(col(\"entry.resource.resourceType\") == \"MedicationRequest\").select(\n",
    "        col(\"entry.resource.id\").alias(\"id\"),\n",
    "        regexp_replace(col(\"entry.resource.subject.reference\"), \"urn:uuid:\", \"\").alias(\"patient_id\"),\n",
    "        col(\"entry.resource.medicationCodeableConcept.text\").alias(\"medication_text\"),\n",
    "        col(\"entry.resource.authoredOn\").cast(DateType()).alias(\"authored_on\")\n",
    "    )\n",
    "\n",
    "    # Escrever os DataFrames no PostgreSQL retirando os já existentes\n",
    "    df_to_insert_patients = patients_df.join(df_existing_patients, patients_df[\"id\"] == df_existing_patients[\"id\"], \"left_anti\")\n",
    "    df_to_insert_patients.write.jdbc(url=jdbc_url, table=\"patients\", mode=\"append\", properties=properties)\n",
    "\n",
    "    df_to_insert_conditions = conditions_df.join(df_existing_conditions, conditions_df[\"id\"] == df_existing_conditions[\"id\"], \"left_anti\")\n",
    "    df_to_insert_conditions.write.jdbc(url=jdbc_url, table=\"conditions\", mode=\"append\", properties=properties)\n",
    "\n",
    "    df_to_insert_medications = medications_df.join(df_existing_medications, medications_df[\"id\"] == df_existing_medications[\"id\"], \"left_anti\")\n",
    "    df_to_insert_medications.write.jdbc(url=jdbc_url, table=\"medication_requests\", mode=\"append\", properties=properties)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Verificar as tabelas e caso não existam, criá-las"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Executando sql/1 - create_table_patients.sql...\n",
      "SQL executado com sucesso: sql/1 - create_table_patients.sql\n",
      "Executando sql/2 - create_table_conditions.sql...\n",
      "SQL executado com sucesso: sql/2 - create_table_conditions.sql\n",
      "Executando sql/3 - create_table_medication_requests.sql...\n",
      "SQL executado com sucesso: sql/3 - create_table_medication_requests.sql\n",
      "Todos os arquivos SQL foram executados.\n"
     ]
    }
   ],
   "source": [
    "# Diretório onde estão os arquivos SQL\n",
    "sql_directory = \"sql/\"\n",
    "\n",
    "# Listar todos os arquivos SQL na pasta\n",
    "sql_files = sorted(glob.glob(os.path.join(sql_directory, \"*.sql\")))  # Ordena para manter a sequência\n",
    "\n",
    "# Função para executar o SQL usando JDBC\n",
    "def execute_sql_file(sql_file, stmt):\n",
    "    with open(sql_file, \"r\", encoding=\"utf-8\") as file:\n",
    "        sql_script = file.read()\n",
    "\n",
    "    print(f\"Executando {sql_file}...\")\n",
    "    # Executa o script inteiro como uma única string\n",
    "    try:\n",
    "        stmt.execute(sql_script)\n",
    "        print(f\"SQL executado com sucesso: {sql_file}\")\n",
    "    except Exception as sql_err:\n",
    "        print(f\"Erro ao executar o comando do arquivo {sql_file}: {sql_err}\")\n",
    "\n",
    "# Iniciar conexão JDBC com o banco de dados\n",
    "try:\n",
    "    # Iniciar a conexão JDBC\n",
    "    conn = spark._sc._gateway.jvm.java.sql.DriverManager.getConnection(\n",
    "        jdbc_url, properties[\"user\"], properties[\"password\"]\n",
    "    )\n",
    "\n",
    "    # Habilita autocommit para evitar problemas de transação\n",
    "    conn.setAutoCommit(True)\n",
    "\n",
    "    stmt = conn.createStatement()\n",
    "\n",
    "    # Loop pelos arquivos SQL e executa cada um\n",
    "    if not sql_files:\n",
    "        print(\"Nenhum arquivo SQL encontrado na pasta 'sql/'.\")\n",
    "    else:\n",
    "        for sql_file in sql_files:\n",
    "            execute_sql_file(sql_file, stmt)\n",
    "\n",
    "    # Fechar conexão\n",
    "    stmt.close()\n",
    "    conn.close()\n",
    "    print(\"Todos os arquivos SQL foram executados.\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"Erro ao executar os arquivos SQL: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Processar os arquivos em lotes com tamanho pré-definido"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processando lote de 0 a 999 (tamanho do lote: 1000)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processando lote de 1000 a 1179 (tamanho do lote: 180)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "df_existing_patients = spark.read.jdbc(jdbc_url, table=\"public.patients\", properties=properties)\n",
    "df_existing_conditions = spark.read.jdbc(jdbc_url, table=\"public.conditions\", properties=properties)\n",
    "df_existing_medications = spark.read.jdbc(jdbc_url, table=\"public.medication_requests\", properties=properties)\n",
    "\n",
    "for i in range(0, len(json_files), batch_size):\n",
    "    batch = json_files[i:i + batch_size]\n",
    "    print(f\"Processando lote de {i} a {i + len(batch) - 1} (tamanho do lote: {len(batch)})\")\n",
    "    process_batch(batch, df_existing_patients, df_existing_conditions, df_existing_medications)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Encerrar a sessão Spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "minsait_venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
